# presenta_ai/adk_logic/callbacks.py (ä¿®æ­£ç‰ˆ)

from typing import Callable, Optional, Awaitable
from google.adk.agents.callback_context import CallbackContext
from google.adk.models import LlmResponse, LlmRequest

# ADKãŒè¦æ±‚ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã®å‹ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’å®šç¾©
BeforeAgentCallback = Optional[Callable[[CallbackContext], Awaitable[None]]]

def create_progress_notifier_callback(progress_callback: Optional[Callable[[str], None]]) -> BeforeAgentCallback:
    """
    UIã«é€²æ—ã‚’é€šçŸ¥ã™ã‚‹ãŸã‚ã® `before_agent_callback` é–¢æ•°ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    ã“ã®é–¢æ•°ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œå‰ã«ç‰¹å®šã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’UIã«é€ä¿¡ã—ã¾ã™ã€‚
    (å‚ç…§: docs/callbacks/types-of-callbacks.md)

    Args:
        progress_callback: UIã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ›´æ–°ã™ã‚‹ãŸã‚ã®åŒæœŸé–¢æ•°ã€‚

    Returns:
        ADKã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«æ¸¡ã™ã“ã¨ãŒã§ãã‚‹éåŒæœŸã®`before_agent_callback`é–¢æ•°ã€‚
        progress_callbackãŒNoneã®å ´åˆã¯Noneã‚’è¿”ã™ã€‚
    """
    if not progress_callback:
        return None

    async def before_agent_callback(ctx: CallbackContext) -> None:
        """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œå‰ã«UIã«é€²æ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã™ã‚‹"""
        agent_name = ctx.agent.name
        message = ""
        
        # ç¾åœ¨ã®Stateã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é¸æŠæƒ…å ±ã‚’å–å¾—
        # (å‚ç…§: docs/sessions/state.md)
        selected_configs = ctx.session.state.get("selected_configs", {})

        if agent_name == "DocumentAnalyzerAgent":
            message = "ãƒ—ãƒ¬ã‚¼ãƒ³è³‡æ–™ã®è§£æã‚’é–‹å§‹ã—ã¾ã™... ğŸ“„"
        elif agent_name == "ReviewerParallelAgent":
            message = "å„å°‚é–€å®¶ã«ã‚ˆã‚‹ä¸¦è¡Œãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’é–‹å§‹ã—ã¾ã™... ğŸ‘¥"
        elif agent_name == "LogicCriticAgent":
            selection = selected_configs.get("logic_critic")
            mode = "è¾›å£ãƒ¢ãƒ¼ãƒ‰" if selection == "strict" else "å¯„ã‚Šæ·»ã„ãƒ¢ãƒ¼ãƒ‰"
            message = f"ãƒ­ã‚¸ãƒƒã‚¯æ‰¹è©•å®¶ ({mode}) ãŒãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­ã§ã™... ğŸ¤”"
        elif agent_name == "AudiencePersonaAgent":
            selection = selected_configs.get("audience_persona")
            mode = "æ‡ç–‘çš„ãªè´è¡†" if selection == "skeptical" else "åˆå¿ƒè€…ãªè´è¡†"
            message = f"è´è¡†ãƒšãƒ«ã‚½ãƒŠ ({mode}) ãŒãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­ã§ã™... ğŸ§"
        elif agent_name == "ReportSynthesizerAgent":
            message = "å„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’çµ±åˆã—ã€æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ã„ã¾ã™... ğŸ“"
        elif agent_name == "QnaGeneratorAgent":
            message = "æƒ³å®šå•ç­”é›†ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™... ğŸ’¬"
        
        if message:
            # UIã¸ã®é€šçŸ¥ã¯åŒæœŸå¾…ã¡å—ã‘ã—ãªã„
            progress_callback(message)

    return before_agent_callback

def get_mime_type(file_path: str) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰MIMEã‚¿ã‚¤ãƒ—ã‚’æ¨æ¸¬ã™ã‚‹ã€‚PPTXã«ã‚‚å¯¾å¿œã€‚"""
    mime_type, _ = mimetypes.guess_type(file_path)
    if mime_type is None:
        if file_path.lower().endswith('.pptx'):
            return 'application/vnd.openxmlformats-officedocument.presentationml.presentation'
        # ä¸æ˜ãªå ´åˆã¯æ±ç”¨çš„ãªMIMEã‚¿ã‚¤ãƒ—ã‚’è¿”ã™
        return 'application/octet-stream'
    return mime_type


async def add_document_to_request_callback(
    context: CallbackContext, llm_request: LlmRequest
) -> Optional[LlmResponse]:
    """
    Stateã‹ã‚‰GCSãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—ã—ã€LLMãƒªã‚¯ã‚¨ã‚¹ãƒˆã«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ãƒ¼ãƒˆã¨ã—ã¦è¿½åŠ ã™ã‚‹ã€‚
    ã“ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ before_model_callback ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã€‚
    æˆ»ã‚Šå€¤ã¨ã—ã¦Noneã‚’è¿”ã™ã¨ã€å¤‰æ›´ãŒé©ç”¨ã•ã‚ŒãŸllm_requestã§å‡¦ç†ãŒç¶šè¡Œã•ã‚Œã‚‹ã€‚
    """
    logger.info("Executing add_document_to_request_callback...")
    gcs_file_path = context.state.get("gcs_file_path")

    if not gcs_file_path or not isinstance(gcs_file_path, str):
        logger.warning(
            "gcs_file_path not found or invalid in state. Skipping file attachment. "
            f"State: {context.state}"
        )
        return None  # ä½•ã‚‚ã›ãšå‡¦ç†ã‚’ç¶šè¡Œ

    logger.info(f"Found gcs_file_path: {gcs_file_path}")

    try:
        # GCS URIã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ãƒ¼ãƒˆã‚’ä½œæˆ
        mime_type = get_mime_type(gcs_file_path)
        file_part = Part.from_uri(uri=gcs_file_path, mime_type=mime_type)

        # LlmRequestã®partsãƒªã‚¹ãƒˆã«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ãƒ¼ãƒˆã‚’è¿½åŠ 
        if llm_request.contents and llm_request.contents[0].parts:
            # æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
            llm_request.contents[0].parts.append(file_part)
            logger.info("Successfully appended file part to LLM request.")
        else:
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒç©ºã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            logger.warning("llm_request.contents was empty. Reconstructing request.")
            text_part = Part.from_text(context.agent.instruction)
            llm_request.contents = [text_part, file_part]

    except Exception as e:
        logger.error(f"Error creating or appending file part: {e}", exc_info=True)
        # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯ä½•ã‚‚ã—ãªã„ï¼ˆã‚ã‚‹ã„ã¯ã‚¨ãƒ©ãƒ¼ã‚’Stateã«è¨˜éŒ²ã™ã‚‹ï¼‰

    # `None`ã‚’è¿”ã™ã“ã¨ã§ã€å¤‰æ›´ã•ã‚ŒãŸ`llm_request`ã§å‡¦ç†ãŒç¶šè¡Œã•ã‚Œã‚‹
    return None