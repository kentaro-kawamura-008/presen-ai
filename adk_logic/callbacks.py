# presenta_ai/adk_logic/callbacks.py (ä¿®æ­£ç‰ˆ)

from typing import Callable, Optional, Awaitable
from google.adk.agents.callback_context import CallbackContext
from google.adk.models import LlmResponse, LlmRequest
from google import genai
from google.genai import types
from logging import getLogger
logger = getLogger(__name__)

# ADKãŒè¦æ±‚ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã®å‹ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’å®šç¾©
BeforeAgentCallback = Optional[Callable[[CallbackContext], Awaitable[None]]]

def before_agent_callback(callback_context: CallbackContext) -> None:
    """å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œå‰ã«UIã«é€²æ—ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡ã™ã‚‹"""
    agent_name = callback_context.agent_name
    message = ""
    
    # ç¾åœ¨ã®Stateã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é¸æŠæƒ…å ±ã‚’å–å¾—
    # (å‚ç…§: docs/sessions/state.md)
    selected_configs = callback_context.state.get("selected_configs", {})

    if agent_name == "DocumentAnalyzerAgent":
        message = "ãƒ—ãƒ¬ã‚¼ãƒ³è³‡æ–™ã®è§£æã‚’é–‹å§‹ã—ã¾ã™... ğŸ“„"
    elif agent_name == "ReviewerParallelAgent":
        message = "å„å°‚é–€å®¶ã«ã‚ˆã‚‹ä¸¦è¡Œãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’é–‹å§‹ã—ã¾ã™... ğŸ‘¥"
    elif agent_name == "LogicCriticAgent":
        selection = selected_configs.get("logic_critic")
        mode = "è¾›å£ãƒ¢ãƒ¼ãƒ‰" if selection == "strict" else "å¯„ã‚Šæ·»ã„ãƒ¢ãƒ¼ãƒ‰"
        message = f"ãƒ­ã‚¸ãƒƒã‚¯æ‰¹è©•å®¶ ({mode}) ãŒãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­ã§ã™... ğŸ¤”"
    elif agent_name == "AudiencePersonaAgent":
        selection = selected_configs.get("audience_persona")
        mode = "æ‡ç–‘çš„ãªè´è¡†" if selection == "skeptical" else "åˆå¿ƒè€…ãªè´è¡†"
        message = f"è´è¡†ãƒšãƒ«ã‚½ãƒŠ ({mode}) ãŒãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­ã§ã™... ğŸ§"
    elif agent_name == "ReportSynthesizerAgent":
        message = "å„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’çµ±åˆã—ã€æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ã„ã¾ã™... ğŸ“"
    elif agent_name == "QnaGeneratorAgent":
        message = "æƒ³å®šå•ç­”é›†ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™... ğŸ’¬"
    
    if message:
        # UIã¸ã®é€šçŸ¥ã¯åŒæœŸå¾…ã¡å—ã‘ã—ãªã„
        print(message)

    return None



def add_document_to_request_callback(
    callback_context: CallbackContext, llm_request: LlmRequest
) -> Optional[LlmResponse]:
    """
    Stateã‹ã‚‰GCSãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—ã—ã€LLMãƒªã‚¯ã‚¨ã‚¹ãƒˆã«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ãƒ¼ãƒˆã¨ã—ã¦è¿½åŠ ã™ã‚‹ã€‚
    ã“ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ before_model_callback ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ã€‚
    æˆ»ã‚Šå€¤ã¨ã—ã¦Noneã‚’è¿”ã™ã¨ã€å¤‰æ›´ãŒé©ç”¨ã•ã‚ŒãŸllm_requestã§å‡¦ç†ãŒç¶šè¡Œã•ã‚Œã‚‹ã€‚
    """
    logger.info("Executing add_document_to_request_callback...")
    gcs_file_path = callback_context.state.get("gcs_file_path")

    if not gcs_file_path or not isinstance(gcs_file_path, str):
        logger.warning(
            "gcs_file_path not found or invalid in state. Skipping file attachment. "
            f"State: {callback_context.state}"
        )
        return None  # ä½•ã‚‚ã›ãšå‡¦ç†ã‚’ç¶šè¡Œ

    logger.info(f"Found gcs_file_path: {gcs_file_path}")

    try:
        # GCS URIã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ãƒ¼ãƒˆã‚’ä½œæˆ
        file_part = types.Part.from_uri(file_uri=gcs_file_path, mime_type="application/pdf")

        # LlmRequestã®partsãƒªã‚¹ãƒˆã«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ãƒ¼ãƒˆã‚’è¿½åŠ 
        # æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
        llm_request.contents[0].parts.append(file_part)
        logger.info("Successfully appended file part to LLM request.")

    except Exception as e:
        logger.error(f"Error creating or appending file part: {e}", exc_info=True)
        # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯ä½•ã‚‚ã—ãªã„ï¼ˆã‚ã‚‹ã„ã¯ã‚¨ãƒ©ãƒ¼ã‚’Stateã«è¨˜éŒ²ã™ã‚‹ï¼‰

    # `None`ã‚’è¿”ã™ã“ã¨ã§ã€å¤‰æ›´ã•ã‚ŒãŸ`llm_request`ã§å‡¦ç†ãŒç¶šè¡Œã•ã‚Œã‚‹
    return None